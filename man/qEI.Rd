% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qEI.R
\name{qEI}
\alias{qEI}
\title{Computes the Multipoint Expected Improvement (qEI) Criterion}
\usage{
qEI(
  x,
  model,
  plugin = NULL,
  type = c("UK", "SK"),
  minimization = TRUE,
  fastCompute = TRUE,
  eps = 1e-05,
  deriv = TRUE,
  envir = NULL
)
}
\arguments{
\item{x}{A numeric matrix representing the set of input points
(one row corresponds to one point) where to evaluate the qEI
criterion.}

\item{model}{An object of class \code{km}.}

\item{plugin}{Optional scalar: if provided, it replaces the
minimum of the current observations.}

\item{type}{\code{"SK"} or \code{"UK"} (by default), depending
whether uncertainty related to trend estimation has to be
taken into account.}

\item{minimization}{Logical specifying if EI is used in
minimiziation or in maximization.}

\item{fastCompute}{Logical. If \code{TRUE}, a fast approximation
method based on a semi-analytic formula is used. See the
reference Marmin (2014) for details.}

\item{eps}{Numeric value of \eqn{epsilon} in the fast computation
trick.  Relevant only if \code{fastComputation} is
\code{TRUE}.}

\item{deriv}{Logical. When \code{TRUE} the dervivatives of the
kriging mean vector and of the kriging covariance (Jacobian
arrays) are computed and stored in the environment given in
\code{envir}.}

\item{envir}{An optional environment specifying where to get
intermediate values calculated in \code{\link{qEI}}.}
}
\value{
The multipoint Expected Improvement, defined as
    \deqn{qEI(X_{new}) := E\left[\{ \min Y(X)  - \min Y(X_{new}) \}_{+}
    \vert Y(X) = y(X) \right],}{qEI(Xnew) := E[{ min Y(X)  - min Y(Xnew) }_{+}
    \vert Y(X) = y(X)],}
where \eqn{X} is the current design of experiments,
\eqn{X_new}{Xnew} is a new candidate design, and
\eqn{Y} is a random process assumed to have generated the
objective function \eqn{y}.
}
\description{
Analytical expression of the multipoint expected
    improvement criterion, also known as the \eqn{q}-\emph{point
    expected improvement} and denoted by \eqn{q}-EI or \code{qEI}.
}
\examples{
\donttest{
set.seed(007)
## Monte-Carlo validation

## a 4-d, 81-points grid design, and the corresponding response
## ============================================================
d <- 4; n <- 3^d
design <- expand.grid(rep(list(seq(0, 1, length = 3)), d))
names(design) <- paste0("x", 1:d)
y <- apply(design, 1, hartman4)

## learning
## ========
model <- km(~1, design = design, response = y, control = list(trace = FALSE))

## pick up 10 points sampled from the 1-point expected improvement
## ===============================================================
q <- 10
X <- sampleFromEI(model, n = q)

## simulation of the minimum of the kriging random vector at X
## ===========================================================
t1 <- proc.time()
newdata <- as.data.frame(X)
colnames(newdata) <- colnames(model@X)

krig  <- predict(object = model, newdata = newdata, type = "UK",
                 se.compute = TRUE, cov.compute = TRUE)
mk <- krig$mean
Sigma.q <- krig$cov
mychol <- chol(Sigma.q)
nsim <- 300000
white.noise <- rnorm(n = nsim * q)
minYsim <- apply(crossprod(mychol, matrix(white.noise, nrow = q)) + mk,
                 MARGIN = 2, FUN = min)

## simulation of the improvement (minimization)
## ============================================
qImprovement <- min(model@y) - minYsim
qImprovement <- qImprovement * (qImprovement > 0)

## empirical expectation of the improvement and confidence interval (95\%)
## ======================================================================
EIMC <- mean(qImprovement)
seq <- sd(qImprovement) / sqrt(nsim)
confInterv <- c(EIMC - 1.96 * seq, EIMC + 1.96 * seq)

## evaluate time
## =============
tMC <- proc.time() - t1

## MC estimation of the qEI
## ========================
print(EIMC) 

## qEI with analytical formula and with fast computation trick
## ===========================================================
tForm <- system.time(qEI(X, model, fastCompute = FALSE))
tFast <- system.time(qEI(X, model))

rbind("MC" = tMC, "form" = tForm, "fast" = tFast)

}

}
\references{
C. Chevalier and D. Ginsbourger (2014) Learning and Intelligent
Optimization - 7th International Conference, Lion 7, Catania,
Italy, January 7-11, 2013, Revised Selected Papers, chapter Fast
computation of the multipoint Expected Improvement with
applications in batch selection, pages 59-69, Springer.

D. Ginsbourger, R. Le Riche, L. Carraro (2007), A Multipoint
Criterion for Deterministic Parallel Global Optimization based on
Kriging. The International Conference on Non Convex Programming,
2007.

S. Marmin (2014). Developpements pour l'evaluation et la
maximisation du critere d'amelioration esperee multipoint en
optimisation globale. Master's thesis, Mines Saint-Etienne
(France) and University of Bern (Switzerland).

D. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is
well-suited to parallelize optimization (2010), In Lim Meng Hiot,
Yew Soon Ong, Yoel Tenne, and Chi-Keong Goh, editors,
\emph{Computational Intelligence in Expensive Optimization
Problems}, Adaptation Learning and Optimization, pages
131-162. Springer Berlin Heidelberg.

J. Mockus (1988), \emph{Bayesian Approach to Global
Optimization}. Kluwer academic publishers.

M. Schonlau (1997), \emph{Computer experiments and global
optimization}, Ph.D. thesis, University of Waterloo.
}
\seealso{
\code{\link{EI}}
}
\author{
Sebastien Marmin, Clement Chevalier and David Ginsbourger.
}
\keyword{models}
\keyword{optimization}
\keyword{parallel}
