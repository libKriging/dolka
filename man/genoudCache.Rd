% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/genoudCache.R
\name{genoudCache}
\alias{genoudCache}
\title{Run the \code{genoud} Optimization Function with a Cached
    Gradient}
\usage{
genoudCache(
  fn,
  nvars,
  max = FALSE,
  pop.size = 1000,
  max.generations = 100,
  wait.generations = 10,
  hard.generation.limit = TRUE,
  starting.values = NULL,
  MemoryMatrix = TRUE,
  Domains = NULL,
  default.domains = 10,
  solution.tolerance = 0.001,
  boundary.enforcement = 0,
  lexical = FALSE,
  gradient.check = TRUE,
  BFGS = TRUE,
  data.type.int = FALSE,
  hessian = FALSE,
  unif.seed = round(runif(1, 1, 2147483647L)),
  int.seed = round(runif(1, 1, 2147483647L)),
  print.level = 2,
  share.type = 0,
  instance.number = 0,
  output.path = "stdout",
  output.append = FALSE,
  project.path = NULL,
  P1 = 50,
  P2 = 50,
  P3 = 50,
  P4 = 50,
  P5 = 50,
  P6 = 50,
  P7 = 50,
  P8 = 50,
  P9 = 0,
  P9mix = NULL,
  BFGSburnin = 0,
  BFGSfn = NULL,
  BFGShelp = NULL,
  control = list(),
  optim.method = ifelse(boundary.enforcement < 2, "BFGS", "L-BFGS-B"),
  transform = FALSE,
  debug = FALSE,
  cluster = FALSE,
  balance = FALSE,
  ...
)
}
\arguments{
\item{fn}{A function to be minimized. This function should return
a list with two named numeric elements \code{objective} and
\code{gradient} representing the value of the objective
(length 1) and the gradient (length \eqn{d}).}

\item{nvars, max, pop.size, max.generations}{See \code{\link[rgenoud]{genoud}}.}

\item{wait.generations, hard.generation.limit}{See \code{\link[rgenoud]{genoud}}.}

\item{starting.values}{See \code{\link[rgenoud]{genoud}}.}

\item{MemoryMatrix}{See \code{\link[rgenoud]{genoud}}.}

\item{Domains}{See \code{\link[rgenoud]{genoud}}.}

\item{default.domains}{See \code{\link[rgenoud]{genoud}}.}

\item{solution.tolerance}{See \code{\link[rgenoud]{genoud}}.}

\item{boundary.enforcement}{See \code{\link[rgenoud]{genoud}}.}

\item{lexical}{See \code{\link[rgenoud]{genoud}}.}

\item{gradient.check}{See \code{\link[rgenoud]{genoud}}.}

\item{BFGS}{See \code{\link[rgenoud]{genoud}}.}

\item{data.type.int}{See \code{\link[rgenoud]{genoud}}.}

\item{hessian}{See \code{\link[rgenoud]{genoud}}.}

\item{unif.seed}{See \code{\link[rgenoud]{genoud}}.}

\item{int.seed}{See \code{\link[rgenoud]{genoud}}.}

\item{print.level}{See \code{\link[rgenoud]{genoud}}.}

\item{share.type}{See \code{\link[rgenoud]{genoud}}.}

\item{instance.number}{See \code{\link[rgenoud]{genoud}}.}

\item{output.path}{See \code{\link[rgenoud]{genoud}}.}

\item{output.append}{See \code{\link[rgenoud]{genoud}}.}

\item{project.path}{See \code{\link[rgenoud]{genoud}}.}

\item{P1, P2, P3, P4, P5, P6}{See \code{\link[rgenoud]{genoud}}.}

\item{P7, P8, P9, P9mix}{See \code{\link[rgenoud]{genoud}}.}

\item{BFGSburnin}{See \code{\link[rgenoud]{genoud}}.}

\item{BFGSfn}{See \code{\link[rgenoud]{genoud}}.}

\item{BFGShelp}{See \code{\link[rgenoud]{genoud}}.}

\item{control}{See \code{\link[rgenoud]{genoud}}.}

\item{optim.method}{See \code{\link[rgenoud]{genoud}}.}

\item{transform}{See \code{\link[rgenoud]{genoud}}.}

\item{debug}{See \code{\link[rgenoud]{genoud}}.}

\item{cluster}{See \code{\link[rgenoud]{genoud}}.}

\item{balance}{See \code{\link[rgenoud]{genoud}}.}

\item{...}{Further arguments to be passed to the function given in
\code{fn}.}
}
\value{
The result of a call to \code{\link[rgenoud]{genoud}}.
}
\description{
Run the \code{\link[rgenoud]{genoud}} function from
    the \pkg{rgenoud} package with the objective and the gradient
    defined in \emph{one} function. The result is global
    minimization of a real-valued function of a numeric vector
    with length \eqn{d} to be provided by \code{fn}. The R
    function \code{fn} must provide the values of the objective
    and its gradient.
}
\note{
The optimization is expected to be faster than
    \code{\link[rgenoud]{genoud}} if the function and its gradient
    are costly to evaluate separately. However this may not be the
    case for quite simple functions, due to the cost of setting
    the "cache" mechanism.
}
\section{Caution}{
 This function is hightly experimental and has
    not been tested enough yet.
}

\examples{

\dontrun{
## Note that in this example, gradient caching would not be worth it.

dom <- cbind(lower = rep(0, 2), upper = rep(0, 2))
library(rgenoud)

## emulate a costly-to-evaluate-alone gradient
## ===========================================
braninDer <- function(x) {
   Sys.sleep(0.01)
   braninGrad(x)$gradient
}

## separate objective and gradient functions
## =========================================
te <- system.time(res <- genoud(fn = branin, nvars = 2, Domains = dom,
                                gr = braninDer))

## gradient "cached"
## ================
teCache <- system.time(resCache <- genoudCache(fn = braninGrad, nvars = 2,
                                               Domains = dom))
rbind("genoud" = te, "genoudCache" = teCache)
c("genoud" = res$value, "genoudCache" = resCache$value)
}
}
